{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src import preparedata\n",
    "from src import modelareaegpdcombined as modelarea\n",
    "from src import inferenceegpd as inference\n",
    "from src import savehazard\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices(device_type=None))\n",
    "params = json.load(open(\"params/paramsegpdcombined.json\", \"r\"))\n",
    "weights = \"savedweights/final_model20230823V1egpdcombined.h5\"  # \"checkpoints/\"#\"savedweights/final_model20230705V2.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su = gpd.read_file(\"Data/SlopeUnits/SlopeUnits_V3.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check landslide size distribution\n",
    "all_area = np.load(\"Data/Inventory/all_area.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(all_area, [0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((all_area < 0.40).mean())\n",
    "print((all_area < 2.0).mean())\n",
    "print((all_area < 10.0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landslidehazard = modelarea.lhmodel(params[\"modelparam\"])\n",
    "landslidehazard.preparemodel()\n",
    "hazcols = []\n",
    "\n",
    "\n",
    "def extractRp(\n",
    "    su=su,\n",
    "    models=[0.4, 2.0, 10.0],  # [0.01760698, 0.42372807, 4.24535745],#,\n",
    "    scenarios=[\"SSP245\", \"SSP585\"],\n",
    "    rps=[5, 10, 15, 20],\n",
    "):\n",
    "    first = True\n",
    "    for model in models:\n",
    "        for scenario in scenarios:\n",
    "            for rp in rps:\n",
    "                dataset = preparedata.readGPDData(params[\"dataprepinargs\"])\n",
    "                dataset.preparedataclimate(rp=rp, model=\"CIMP6\", scenario=scenario)\n",
    "                haz = inference.inferenceLHProb(\n",
    "                    model=landslidehazard.model,\n",
    "                    model_weights=weights,\n",
    "                    xdata=dataset.Xinference.astype(\"float32\"),\n",
    "                    ep=model,\n",
    "                )\n",
    "                print(haz.shape)\n",
    "                print(haz)\n",
    "                ids = dataset.InferenceID\n",
    "                hazcol = f\"{rp}_{scenario}_{str(model).replace('.','_')}\"\n",
    "                if first:\n",
    "                    first = False\n",
    "                    hazarddata = pd.DataFrame({\"cat\": ids, hazcol: haz.flatten()})\n",
    "                    # hazarddata=np.expand_dims(haz,axis=-1)\n",
    "                else:\n",
    "                    df = pd.DataFrame({hazcol: haz.flatten()})\n",
    "                    hazarddata = pd.concat([hazarddata, df], axis=1, join=\"inner\")\n",
    "                    # hazarddata=np.concatenate((hazarddata,np.expand_dims(haz,axis=-1)),axis=-1)\n",
    "                print(hazarddata.shape)\n",
    "\n",
    "                #\n",
    "                # df=pd.DataFrame({'cat':ids,hazcol:haz})\n",
    "                # projections=su.merge(df,on='cat')\n",
    "                hazcols.append(hazcol)\n",
    "                # su=projections\n",
    "                # break\n",
    "            # break\n",
    "        # break\n",
    "    return hazarddata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haz = extractRp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haz.columns = [\"cat\"] + hazcols\n",
    "haz = haz.drop_duplicates(subset=[\"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = su.merge(haz, on=\"cat\")\n",
    "results.to_file(\"Data/ResultsV5/ClimateProjectionsV555555ALT_prob.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict current situations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landslidehazard = modelarea.lhmodel(params[\"modelparam\"])\n",
    "landslidehazard.preparemodel()\n",
    "hazcols = []\n",
    "rps = [5, 10, 15, 20]\n",
    "first = True\n",
    "models = [0.4, 2.0, 10.0]  # [0.01760698, 0.42372807, 4.24535745]#[5,50,95]\n",
    "for model in models:\n",
    "    for rp in rps:\n",
    "        dataset = preparedata.readGPDData(params[\"dataprepinargs\"])\n",
    "        dataset.preparedatainference(rp)\n",
    "        haz = inference.inferenceLHProb(\n",
    "            model=landslidehazard.model,\n",
    "            model_weights=weights,\n",
    "            xdata=dataset.Xinference.astype(\"float32\"),\n",
    "            ep=model,\n",
    "        )\n",
    "        ids = dataset.InferenceID\n",
    "        hazcol = f\"{rp}_{str(model).replace('.','_')}\"\n",
    "        if first:\n",
    "            first = False\n",
    "            hazarddatacr = pd.DataFrame({\"cat\": ids, hazcol: haz.flatten()})\n",
    "            # hazarddata=np.expand_dims(haz,axis=-1)\n",
    "        else:\n",
    "            df = pd.DataFrame({hazcol: haz.flatten()})\n",
    "            hazarddatacr = pd.concat([hazarddatacr, df], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_haz = su.merge(hazarddatacr, on=\"cat\")\n",
    "current_haz = current_haz.drop_duplicates(subset=[\"cat\"])\n",
    "current_haz.to_file(\n",
    "    \"Data/ResultsV5/Current_predictionsV5555ALT_prob.gpkg\", driver=\"GPKG\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all results to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"cat\"] + current_haz.columns[3:].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sub = results\n",
    "all_results = result_sub.merge(\n",
    "    current_haz[[\"cat\"] + current_haz.columns[3:].to_list()], on=\"cat\"\n",
    ")\n",
    "\n",
    "for quantile in [0.4, 2.0, 10.0]:  # [0.01760698, 0.42372807, 4.24535745]:\n",
    "    for year in [\"5\", \"10\", \"15\", \"20\"]:\n",
    "        all_results[f\"r_SSP245_{year}_{str(quantile).replace('.','_')}\"] = 100 * (\n",
    "            (\n",
    "                all_results[\n",
    "                    f\"{year}_SSP245_{str(quantile).replace('.','_')}\"\n",
    "                ].to_numpy()\n",
    "                - all_results[f\"{year}_{str(quantile).replace('.','_')}\"].to_numpy()\n",
    "            )\n",
    "            / (all_results[f\"{year}_{str(quantile).replace('.','_')}\"].to_numpy())\n",
    "        )\n",
    "        all_results[f\"r_SSP585_{year}_{str(quantile).replace('.','_')}\"] = 100 * (\n",
    "            (\n",
    "                all_results[\n",
    "                    f\"{year}_SSP585_{str(quantile).replace('.','_')}\"\n",
    "                ].to_numpy()\n",
    "                - all_results[f\"{year}_{str(quantile).replace('.','_')}\"].to_numpy()\n",
    "            )\n",
    "            / (all_results[f\"{year}_{str(quantile).replace('.','_')}\"].to_numpy())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "all_results.replace([np.nan, -np.nan], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.to_file(\"Data/ResultsV5/all_resultsV56666666666ALT_prob.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
